{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "from pyspark.ml.feature import StandardScaler, OneHotEncoder, StringIndexer, Imputer, VectorAssembler\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark context\n",
    "sc = SparkContext(\"local\", \"pipelines\")\n",
    "# Get spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Get the data and place it in a spark dataframe\n",
    "data = spark.read.format(\"csv\").option(\"sep\", \";\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(\n",
    "    \"../../chapter1/stream-classifier/data/bank/bank.csv\")\n",
    "\n",
    "# map target to numerical category\n",
    "data = data.withColumn('label', f.when((f.col(\"y\") == \"yes\"), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list for storage stage references\n",
    "stages = []\n",
    "\n",
    "# define the transformation stages for the categorical columns\n",
    "categoricalColumns = [\"job\", \"marital\", \"education\", \"contact\", \"housing\", \"loan\", \"default\", \"day\"]\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # fill some nulls\n",
    "    # data = data.na.fill({categoricalCol:’Unknown’})\n",
    "    # category indexing with string indexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol,\n",
    "                                  outputCol=categoricalCol +\"Index\").setHandleInvalid(\"keep\")  # keep is for unknown categories\n",
    "    # Use onehotencoder to convert cat variables into binary sparseVectors\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol +\"classVec\"])\n",
    "    # Add stages. These are not run here, will be run later\n",
    "    stages += [stringIndexer, encoder]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define impute stage for the numerical columns\n",
    "numericalColumns = [\"age\", \"balance\"]\n",
    "numericalColumnsImputed = [x + \"_imputed\" for x in numericalColumns]\n",
    "imputer = Imputer(inputCols=numericalColumns, outputCols=numericalColumnsImputed)\n",
    "stages += [imputer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define numerical assembler first for scaling\n",
    "numericalAssembler = VectorAssembler(inputCols=numericalColumnsImputed, outputCol='numerical_cols_imputed')\n",
    "stages += [numericalAssembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standard scaler stage for the numerical columns\n",
    "scaler = StandardScaler(inputCol='numerical_cols_imputed', outputCol=\"numerical_cols_imputed_scaled\")\n",
    "stages += [scaler] # already a list so no need for brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform assembly stage to bring together features\n",
    "assemblerInputs = [c+\"classVec\" for c in categoricalColumns]+[\"numerical_cols_imputed_scaled\"]\n",
    "# features contains everything, one hot encoded and numerical\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model stage at the end of the pipeline\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "stages += [lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the entire pipeline and fit and transform on the data\n",
    "# Random train test split with seed\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfPipeline = Pipeline().setStages(stages).fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfPipeline.transform(testData).select(\"prediction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mleng] *",
   "language": "python",
   "name": "conda-env-mleng-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
