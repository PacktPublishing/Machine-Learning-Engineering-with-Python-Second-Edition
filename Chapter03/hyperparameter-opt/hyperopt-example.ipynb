{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperOpt\n",
    "from hyperopt import hp, tpe, STATUS_OK, Trials, fmin\n",
    "# sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets\n",
    "\n",
    "from functools import partial # to solve scoping problem when supplying more params' to objective function\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'warm_start': hp.choice('warm_start', [True, False]),\n",
    "    'fit_intercept': hp.choice('fit_intercept', [True, False]),\n",
    "    'tol': hp.uniform('tol', 0.00001, 0.0001),\n",
    "    'C': hp.uniform('C', 0.05, 2.5),\n",
    "    'solver': hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear']),\n",
    "    'max_iter': hp.choice('max_iter', range(10, 500))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params, n_folds, X, y):\n",
    "    \"\"\"Objective function for tuning logistic regression hyperparameters\"\"\"\n",
    "\n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    clf = LogisticRegression(**params, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=n_folds, scoring='f1_macro')\n",
    "\n",
    "    # Extract the best score\n",
    "    max_score = max(scores)\n",
    "\n",
    "    # Loss must be minimized\n",
    "    loss = 1 - max_score\n",
    "\n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "X, y = datasets.make_classification(n_samples=100000, n_features=20,\n",
    "                                    n_informative=2, n_redundant=2)\n",
    "\n",
    "train_samples = 100  # Samples used for training the models\n",
    "\n",
    "X_train = X[:train_samples]\n",
    "X_test = X[train_samples:]\n",
    "y_train = y[:train_samples]\n",
    "y_test = y[train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trials object to track progress\n",
    "trials = Trials()\n",
    "\n",
    "# Optimize\n",
    "best = fmin(\n",
    "    fn=partial(objective, n_folds=n_folds, X=X_train, y=y_train),\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=16,\n",
    "    trials=trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mleng] *",
   "language": "python",
   "name": "conda-env-mleng-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
